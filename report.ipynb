{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python [conda root]","language":"python","name":"conda-root-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.6"},"colab":{"name":"report.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"T8Srq0WZaL8Y","colab_type":"text"},"source":["$\\qquad$ $\\qquad$$\\qquad$  **BBM409 Machine Learning: Assignment 3** <br />\n","$\\qquad$ $\\qquad$$\\qquad$ **Goal: Neural Network**<br />\n","$\\qquad$ $\\qquad$$\\qquad$                   **MELTEM TOKGÖZ** <br />\n","$\\qquad$ $\\qquad$$\\qquad$                     **21527381** <br />\n","$\\qquad$ $\\qquad$$\\qquad$                   **b21527381@cs.hacettepe.edu.tr** <br />"]},{"cell_type":"markdown","metadata":{"id":"sv8FYmfiaL8Z","colab_type":"text"},"source":["# PART I: Theoretical problems"]},{"cell_type":"markdown","metadata":{"id":"1aCv1S4SaL8a","colab_type":"text"},"source":["**Question1**: What are differences between logistic regression and linear regression?  <br />\n","**SOLUTION :**<br />\n","* In linear regression, the outcome (dependent variable) is continuous. It can have any one of an infinite number of possible values. In logistic regression, the outcome (dependent variable) has only a limited number of possible values.<br />\n","* Linear regression gives an equation which is of the form Y = mX + C, means equation with degree 1. However, logistic regression gives an equation which is of the form Y = eX + e-X <br />\n","* Linear regression aims at finding the best-fitting straight line which is also called a regression line. In the above figure, the red diagonal line is the best-fitting straight line and consists of the predicted score on Y for each possible value of X. The distance between the points to the regression line represent the errors.<br />\n","<img src =\"log.png\"><br />\n","* Changing the coefficient leads to change in both the direction and the steepness of the logistic function. It means positive slopes result in an S-shaped curve and negative slopes result in a Z-shaped curve.<br />\n","<img src =\"logreg.png\"><br />"]},{"cell_type":"markdown","metadata":{"id":"WzFMZELWaL8a","colab_type":"text"},"source":["**Question2:** What are differences between logistic regression and naive bayes methods? <br />\n","**SOLUTION:**<br />\n","* The learning mechanism is a bit different between the two models, where Naive Bayes is a generative model and Logistic regression is a discriminative model. What does this mean?<br />  Generative model: Naive Bayes models the joint distribution of the feature X and target Y, and then predicts the posterior probability given as P(y|x)<br /> Discriminative model: Logistic regression directly models the posterior probability of P(y|x) by learning the input to output mapping by minimising the error.<br />\n","* Naïve Bayes: When the training data size is small relative to the number of features, the information/data on prior probabilities help in improving the results<br /> Logistic regression: When the training data size is small relative to the number of features, including regularisation such as Lasso and Ridge regression can help reduce overfitting and result in a more generalised model.<br />\n","* Naïve Bayes assumes all the features to be conditionally independent. So, if some of the features are in fact dependent on each other (in case of a large feature space), the prediction might be poor.<br /> Logistic regression splits feature space linearly, and typically works reasonably well even when some of the variables are correlated.<br />\n","* Summaries: When naive bayes estimates a joint probability from the training data, logistic regression estimates the probability from the training data by minimizing error. The other difference is limitations. When naive bayes can even with less training data, the logistic regression, model estimates may over fit the data. In addition, when the naive bayes assumes all the features are conditionally independent, logistic regression splits feature space linearly.<br />"]},{"cell_type":"markdown","metadata":{"id":"NHwq0C78aL8b","colab_type":"text"},"source":["**Question3:** Which of the following statements are true? <br />\n","**SOLUTION:**<br />\n","* A two layer (one input layer, one output layer; no hidden layer) neural network can represent the XOR function.<br />**False**<br />We must compose multiple logical operations by using a hidden layer to represent the XOR function.<br />XOR not linearly separable.<br /><img src =\"xor2.jpg\"><br />\n","* Any logical function over binary-valued (0 or 1) inputs x1andx2 can be (approximately) represented using some neural network.<br />**True**<br />Since we can build the basic AND, OR, and NOT functions with a two layer network, we can (approximately) represent any logical function by composing these basic functions over multiple layers.<br />\n","* Suppose you have a multi-class classiffication problem with three classes,trained with a 3 layer network. Let a(3)1 = (h(x))1 be the activation of the first output unit and similarly a(3) 2 = (h(x))2 and a(3) 3 = (h(x))3. Then for any input x, it must be the case that that a(3) 1 + a(3) 2 + a(3) 3 = 1.<br />**False**<br />The outputs of a neural network are not probabilities, so their sum need not be 1.<br />\n","* The activation values of the hidden units in a neural network, with the sigmoid activation function applied at every layer, are always in the range (0, 1).<br />**True**<br />\n"]},{"cell_type":"markdown","metadata":{"id":"__4gkvfxaL8b","colab_type":"text"},"source":["**Question4:** How to decide the number of hidden layers and nodes in a hidden layer?  <br />\n","**SOLUTION:**<br />\n","* There is no hard-and-fast rule for this.<br />  The number of hidden nodes you should have is based on a complex relationship between<br /> Number of input and output nodes<br />Amount of training data available<br />Complexity of the function that is trying to be learned<br />The training algorithm<br /> \n","* To minimize the error and have a trained network that generalizes well, you need to pick an optimal number of hidden layers, as well as nodes in each hidden layer. <br />Too few nodes will lead to high error for your system as the predictive factors might be too complex for a small number of nodes to capture <br />Too many nodes will overfit to your training data and not generalize well<br />\n","* The number of hidden nodes in each layer should be somewhere between the size of the input and output layer, potentially the mean.<br />The number of hidden nodes shouldn't need to exceed twice the number of input nodes, as you are probably grossly overfitting at this point.<br />"]},{"cell_type":"code","metadata":{"id":"rotgPTa4aL8c","colab_type":"code","colab":{}},"source":["#********************************************************************************************************************************"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oPhjDt0XaL8g","colab_type":"text"},"source":["# PART II: Classification of Flowers using Neural Network"]},{"cell_type":"markdown","metadata":{"id":"zmaV9_C9aL8g","colab_type":"text"},"source":["In this part of the assignment, I will implement a single layer and multilayer neural network architecture to classify flowers into 5 classses."]},{"cell_type":"markdown","metadata":{"id":"4o7OV87NaL8h","colab_type":"text"},"source":["Firstly, I used to import the libraries I used."]},{"cell_type":"code","metadata":{"id":"XW2SI1BGaL8h","colab_type":"code","colab":{}},"source":["import numpy as np\n","from scipy import io\n","import matplotlib.pyplot as plt\n","import argparse\n","import os"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ETBuitGQaL8k","colab_type":"text"},"source":["With this function I give the hyper parameters I want."]},{"cell_type":"code","metadata":{"id":"dL5X8OnwaL8l","colab_type":"code","colab":{}},"source":["#***********************PARAMETERS*********************************************\n","def argumentParser():\n","    ap = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter)\n","    ap.add_argument(\"-td\", \"--traindata\", default='train.mat')\n","    ap.add_argument(\"-ttd\", \"--testdata\", default='test.mat')\n","    ap.add_argument(\"-i\", \"--inputsize\", default='768')\n","    ap.add_argument(\"-o\", \"--outputsize\", default='5')\n","    ap.add_argument(\"-b\", \"--batchsize\", default='128')\n","    ap.add_argument(\"-e\", \"--epoch\", default='100')\n","    ap.add_argument(\"-lr\", \"--learningrate\", default='0.01')\n","    return vars(ap.parse_args())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q6661OX5aL8o","colab_type":"text"},"source":["I've done a function to read and throw the data into matrices.Here :<br />\n","**label : ** 0-1-2-3-4 so total 5 class X 3000 image <br />\n","**image vector size : **  768 x 3000 image <br />Because it is important to normalize the image values from 0 to 1 (0 için 255), I divided it to 255.<br />There are other techniques to do normalization here.I've specified them in the comment line and I've tried trying my code with other techniques. But I didn't use it because it gave almost the same results."]},{"cell_type":"code","metadata":{"id":"7HuQgMBZaL8o","colab_type":"code","colab":{}},"source":["# **********************READ DATA FUNCTION and NORMALIZATION***********************************************\n","def readData(trainFile, testFile):\n","    train = io.loadmat(trainFile)\n","    test = io.loadmat(testFile)\n","\n","    ###TRAIN DATA###\n","    train_label = np.array(train['y'][0])\n","    train_vector = np.array(train['x']) / 255\n","    #normalization\n","    # train_vector = (train_vector -np.min(train_vector)) / (np.max(train_vector)-np.min(train_vector))\n","    # standartization\n","    # train_vector  = (train_vector - np.mean(train_vector)) / (10.0 * np.std(train_vector))\n","\n","    ###TEST DATA###\n","    test_label = np.array(test['y'][0])\n","    test_vector = np.array(test['x']) / 255\n","    return train_label, train_vector, test_label, test_vector"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_5EQ-Qn_aL8r","colab_type":"text"},"source":["This function to be correct class 1 other class 0 for each image.For example if correct class 2 , [0 0 1 0 0]"]},{"cell_type":"code","metadata":{"id":"XMKbp8OEaL8s","colab_type":"code","colab":{}},"source":["# ***************ONE HOT**************************************************\n","def oneHot(label, class_size):\n","    y = label.shape[0]\n","    label_matrix = np.zeros([y, class_size], dtype=int)\n","    for i in range(y):\n","        correct_class = label[i]\n","        label_matrix[i][correct_class] = 1\n","    return label_matrix"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hwrHs8DMaL8x","colab_type":"text"},"source":["I wanted to try my code in different activation functions, so I defined different activation functions.<br />\n","**1-Sigmoid Function :** The formula is as follows:"]},{"cell_type":"markdown","metadata":{"id":"lC67Fx6zaL8x","colab_type":"text"},"source":["<img src =\"sigmoid.png\"><br />"]},{"cell_type":"code","metadata":{"id":"93ePqAVWaL8y","colab_type":"code","colab":{}},"source":["#**********************SIGMOID FUNCTION****************************************\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","def derivative_sigmoid(x):\n","    return sigmoid(x) * (1 - sigmoid(x))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F9Jw1xQnaL81","colab_type":"text"},"source":["**2-RELU Function :**  The formula is as follows: and result accuracy 16.06648199445983"]},{"cell_type":"markdown","metadata":{"id":"EC__HvUHaL81","colab_type":"text"},"source":["<img src =\"relu.png\"><br />"]},{"cell_type":"code","metadata":{"id":"1eFe4PxiaL82","colab_type":"code","colab":{}},"source":["# ************************ReLu FUNCTION****************************************\n","def relu(x):\n","    return np.maximum(x, 0)\n","\n","def derivative_relu(x):\n","    x[x <= 0] = 0\n","    x[x > 0] = 1\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LiO0k7RVaL84","colab_type":"text"},"source":["**3- Tanh Function :**   The formula is as follows:"]},{"cell_type":"markdown","metadata":{"id":"j5_w4sHRaL85","colab_type":"text"},"source":["<img src =\"tanh.png\"><br />"]},{"cell_type":"code","metadata":{"id":"KBNU7K-IaL85","colab_type":"code","colab":{}},"source":["#*************************TANH FUNCTION****************************************\n","def tanh(X):\n","    return np.tanh(X)\n","    \n","def derivative_tanh(x):\n","    return 1.0 - np.tanh(x) ** 2"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MoLtMC_5aL89","colab_type":"text"},"source":["**4- Softmax Function : **  The formula is as follows:"]},{"cell_type":"markdown","metadata":{"id":"csMbPqq4aL8-","colab_type":"text"},"source":["<img src =\"soft.jpg\"><br />"]},{"cell_type":"code","metadata":{"id":"5D0hayX4aL8-","colab_type":"code","colab":{}},"source":["# ***************SOFTMAX FUNCTION**********************************************\n","def softmax(out_array):\n","    for i in range(out_array.shape[0]):\n","        out_array[i] = np.exp(out_array[i]) / np.sum(np.exp(out_array[i]), axis=0)\n","    return out_array\n","\n","def derivative_softmax(preds):\n","    return preds * (1 - preds)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QRerWi-2aL9B","colab_type":"text"},"source":["**LOSS FUNCTION**\n","The loss function basically calculates how different the prediction of the model is from the ground truth. \n","As loss function, I used sum of negative log-likelihood of the correct labels in this assignment."]},{"cell_type":"code","metadata":{"id":"qsAg9apUaL9C","colab_type":"code","colab":{}},"source":["#*****************LOSS FUNCTION************************************************\n","def loss(train_label, a1, size):\n","    result = -np.sum(np.multiply(train_label, np.log(a1)) + np.multiply(1 - train_label, np.log(1 - a1))) / size\n","    return result"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_lNvoQcXaL9F","colab_type":"text"},"source":["**The parts so far consisted of activation functions, loss function and data reading  for  I used both in single layer and in multilayer**<br />\n","Now I'm going to tell you how I defined a model on the single layer."]},{"cell_type":"markdown","metadata":{"id":"NToOcSXbaL9G","colab_type":"text"},"source":["# 1 Single Layer Neural Network"]},{"cell_type":"markdown","metadata":{"id":"KzSMW0TTaL9H","colab_type":"text"},"source":["In the code I wrote below, I will tell you the stages of the single layer one by one.<br />\n","* As a first step, I gave the initial values of weigt and bias.I gave them all 0 in bias, I used random function in weight.<br />\n","*  In the first place I give the necessary hyper parameters that call the function s_nn with args in main function. And I called s_nn function from main.<br />"]},{"cell_type":"markdown","metadata":{"id":"mL84c0ZWaL9H","colab_type":"text"},"source":["**EPOCH COMMENT**<br />\n","While the model is being trained, all of the data is not included in the training at the same time. They take part in a number of parts. The first part is trained, the performance of the model is tested and the weights are updated with backpropagation. Then the model is re-trained with the new training set and the weights are updated again. This process is repeated in each training step to calculate the most suitable weight values for the model. Each of these training steps is called “epoch\".<br />\n","* As the most suitable weight values were calculated step by step in my assignment, the performance was lower in the first epochs, and as the number of epoch increased, the performance increased. However, after a certain step, the learning status of my model will be considerably reduced.<br />\n","* For example 3000 epoch result: I print a accuracy value for every 100 epoch.<br />\n","<img src =\"epoch.png\"><br />\n","*  For example 500 epoch result: I print a accuracy value for every 100 epoch.<br />\n","<img src =\"500epoch.png\"><br />\n"]},{"cell_type":"markdown","metadata":{"id":"PWWx9nbwaL9I","colab_type":"text"},"source":["**BATCH COMMENT**<br />\n","When designing the model, the value defined as mini-batch parameter means that the model will process the how many data at the same time.When we set the mini-batch value to 1 as the smallest value, we do the same job as  “schoastic gredient decent”. So in each iteration, we only process one data.In case the mini-bath value is equal to the number of all elements in the training set; all the data in the training set will enter training and the process will be the same as “batch gredient descent\"<br />\n","* When I apply a mini-batch, there are fluctuations in the error value, as seen in the graph.<br />\n","** Below is the loss chart of my code which I tried with 3 different mini batch values.**<br />\n","* 1- mini batch value = 10<br /> It causes the model to learn the noise. So the zigzags in the chart are multiplying. Because different data is used each time during the learning process.<br /><img src =\"10.png\"><br />\n","* 2- mini batch value = 3000<br />The model learns less noise. Learning takes a little longer because it will process all the data at the same time.<br /><img src =\"3000.png\"><br />\n","* 3- mini batch value = 128 <br /> In selecting the mini-batch value, I have set a value that is neither too small nor too large between the most appropriate value 1 and the number of all data in the training set.<br /><img src =\"128.png\"><br />\n"]},{"cell_type":"markdown","metadata":{"id":"LdvfGNseaL9I","colab_type":"text"},"source":["Now, I implemented to single layer neural network algoritm.\n","First, I'm doing feed forward."]},{"cell_type":"markdown","metadata":{"id":"RGT6VXwxaL9J","colab_type":"text"},"source":["**FEED FORWARD COMMENT**<br />\n","I used follows formula this function.<br />\n","Formula : oi = wijxj + bi (here w=weight b=bias)<br />\n","Then I sent the output value from this process to the activation function.<br />"]},{"cell_type":"markdown","metadata":{"id":"gaE0wj3WaL9K","colab_type":"text"},"source":["**ACTIVATION FUNCTION COMMENT**<br />\n","I have tried different activation functions in my code and got results.<br />\n","Accuracy values of the activation functions I use are as follows.<br />\n","I found that the best result is softmax funtion.That's why I'm using it in my code.<br />\n","**Softmax Function : ** result accuracy 31.57894736842105<br />\n","**Sigmoid Function : ** result accuracy 30.886426592797783<br />\n","**Tanh Function    : **  result accuracy 16.06648199445983<br />\n","**RELU Function    : **  result accuracy 16.06648199445983<br />"]},{"cell_type":"markdown","metadata":{"id":"LOUWMC7paL9K","colab_type":"text"},"source":["**BACKPROPAGATION COMMENT**<br />\n","In the backpropagation process, this update is called chain rule.<br />\n","Finding the difference by taking back the derivative and multiplying the difference value by the learning rate is calculated by subtracting the result from the weight values and calculating the new weight value.<br />\n","I find the new weight and bias values by taking back the derivative and multiplying them with the learning rate.First I take the derivative of the loss function and then the activation function I use.Then I am update weight and bias.<br />\n"]},{"cell_type":"markdown","metadata":{"id":"LnHPgZ7zaL9L","colab_type":"text"},"source":["**LEARNING RATE COMMENT**<br />\n","learning rate value 0.005 between 0.02<br />\n","Having a high learning rate means I'm very impressed by the data.<br />\n","I have tried different learning rate in my code and got results.<br />\n","* Learning rate : 0.005 Accuracy :  29.085872576177284<br />\n","* Learning rate : 0.02 Accuracy  :  29.916897506925206<br />\n","* Learning rate : 0.01 Accuracy  :  30.332409972299168<br />"]},{"cell_type":"markdown","metadata":{"id":"tPwsLtsraL9M","colab_type":"text"},"source":["**ACCURACY COMMENT**<br />\n","I'm applying feed forward to my test dataset with the latest weight and bias values.<br />\n","Then, I compare the test data set with the actual classes and find out how many correct classifications there are.<br />\n","With the following parameters(), I found the best accuracy 30.055401662049864"]},{"cell_type":"markdown","metadata":{"id":"6jxaN53DaL9M","colab_type":"text"},"source":["**NOTE!!!**<br />\n","My Teacher ,I've registered program as a model, but I'm not sure if I'm doing it right, so I'm writing the final parameters I've used against just in case I've already given them by default.<br />\n","batchsize =128<br />\n","epoch = 2000<br />\n","learningrate =0.01<br />\n","Activation function = Softmax<br />\n"]},{"cell_type":"markdown","metadata":{"id":"8dk4uYI_aL9N","colab_type":"text"},"source":["**RESULT**<br />\n","I've divided it into 100 to optimize numbers in many places.<br />\n","The best accuracy is 30.05 in my single layer neural network.\n"]},{"cell_type":"code","metadata":{"id":"yeROA70yaL9N","colab_type":"code","colab":{}},"source":["# ******************************************************************************\n","#initialize weight and bias value\n","def initialize_parameters(input_s,output_s):\n","    initial_b = 0.0 #give zero\n","    initial_w = np.random.randn(input_s, output_s) #give random\n","    return initial_w, initial_b\n","#******************************************************************************\n","def feedforward(train_vector, initial_w, initial_b):\n","    # this formula : oi = wijxj + bi\n","    o1 = train_vector.dot(initial_w) + initial_b\n","    # send to activation function:\n","    a1 = softmax(o1 / 100)\n","    return a1\n","# ******************************************************************************\n","def backpropagation(train_label, train_vector, a1, initial_w, initial_b, learning_rate):\n","    # Backpropagation\n","    da1 = -(np.divide(train_label, a1) - np.divide(1 - train_label, 1 - a1)) / 100\n","    dh1 = da1 * derivative_softmax(a1)\n","    new_weight = train_vector.T.dot(dh1)\n","    new_bias = np.sum(dh1)\n","\n","    # update weight and bias\n","    initial_w -= learning_rate * new_weight\n","    initial_b -= learning_rate * new_bias\n","    return initial_w, initial_b\n","# *****************************************************************************\n","def accuracy(test_vector,test_label,initial_w,initial_b):\n","    preds = softmax((test_vector.dot(initial_w) + initial_b) / 100)\n","    count = 0\n","    for i, j in zip(test_label, np.argmax(preds, axis=1)):\n","        if i == j:\n","            count += 1\n","    print(\"Accuracy : \")\n","    print(count / test_vector.shape[0] * 100)\n","# *****************************************************************************\n","def s_nn(train_vector, train_label, test_vector, test_label, batch_size, initial_w, initial_b, learning_rate,epoch_size):\n","    count = 0\n","    for epoch in range(epoch_size):\n","        g = []\n","        for i in range(0, len(train_vector), batch_size):\n","            size = train_vector[i:i + batch_size].shape[0]\n","            a1 = feedforward(train_vector[i:i + batch_size], initial_w, initial_b)\n","            # send to loss function:\n","            loss_val = loss(train_label[i:i + batch_size], a1, size)\n","            g.append(loss_val)\n","            intial_w, initial_b = backpropagation(train_label[i:i + batch_size], train_vector[i:i + batch_size]\n","                                                  , a1, initial_w, initial_b, learning_rate)\n","            count += 1\n","\n","        if epoch % 100 == 0:\n","            preds = softmax((test_vector.dot(initial_w) + initial_b) / 100)\n","            count = 0\n","            for i, j in zip(test_label, np.argmax(preds, axis=1)):\n","                if i == j:\n","                    count += 1\n","\n","            print(count / test_vector.shape[0] * 100)\n","\n","    accuracy(test_vector, test_label, initial_w, initial_b)\n","     \n","\n","    g.clear\n","    plt.plot(g)\n","    plt.show()\n","# ******************************************************************************  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rd37v0YyaL9R","colab_type":"text"},"source":["**NOTES**<br />\n","My teacher, my code is running normally spyder or pycharm but ipython there is an error with args.If there is an error args here, you should know that you can run my code on other platforms(pycharm, spyder)."]},{"cell_type":"code","metadata":{"id":"5jcwypTSaL9R","colab_type":"code","colab":{}},"source":["def main():\n","    args = argumentParser()\n","    train_label, train_vector, test_label, test_vector = readData(args['traindata'], args['testdata'])\n","    train_label = oneHot(train_label, int(args['outputsize']))\n","    initial_w , initial_b = initialize_parameters(int(args['inputsize']),int(args['outputsize']))         \n","    # other typhical value 0.001, 0.01, 0.03, 0.1, 0.3, 1 etc.\n","    #s_nn(train_vector, train_label, test_vector, test_label, int(args['batchsize']), initial_w, initial_b,float(args['learningrate']), int(args['epoch']))\n","    # save learned parameters\n","    if not os.path.isdir(\"../model\"):\n","        os.mkdir(\"../model\")\n","    np.savez(\"../model/model.npz\", weights=initial_w, bias= initial_b )\n","    print(\"Parameters are saved..\")\n","    # ******************************************************************************\n","\n","if __name__ == '__main__':\n","    main()\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2RyoeLPiaL9V","colab_type":"text"},"source":["# 2-Multi Layer Neural Network"]},{"cell_type":"markdown","metadata":{"id":"0o5UzN5_aL9W","colab_type":"text"},"source":["Here, I did almost the same with the single layer.<br />\n","I could not make the number of layers dynamic, but my code works for 1 input layer and 1 hidden layer.<br />\n","I can dynamically change the number of neurons in the hidden layer. I tried the code with a different number of neurons.<br />\n","And I've observed that when there are more neurons, it gives better accuracy.<br />"]},{"cell_type":"markdown","metadata":{"id":"-NllWuqaaL9W","colab_type":"text"},"source":["With parameters<br />\n","epoch_size = 1000<br />\n","learning_rate = 0.01<br />\n","batch_size = 126<br />\n","output_size = 5<br />\n","input_size = 768<br />\n","hidden_size = 50 <br />\n","\n","** result : ** 25.346260387811636<br />\n","\n","With parameters<br />\n","epoch_size = 3000<br />\n","learning_rate = 0.005<br />\n","batch_size = 256<br />\n","output_size = 5<br />\n","input_size = 768<br />\n","hidden_size = 5<br />\n","\n","** result : ** 25.346260387811636<br />\n"]},{"cell_type":"code","metadata":{"id":"e_hDkNyOaL9X","colab_type":"code","colab":{}},"source":["import numpy as np\n","from scipy import io\n","import matplotlib.pyplot as plt\n","# **********************READ DATA**********************************************\n","def readData(trainFile, testFile):\n","    train = io.loadmat(trainFile)\n","    test = io.loadmat(testFile)\n","    ###TRAIN DATA###\n","    train_label = np.array(train['y'][0])\n","    train_vector = np.array(train['x']) / 255\n","    # train_vector = (train_vector -np.min(train_vector)) / (np.max(train_vector)-np.min(train_vector))\n","    # standardization\n","    # train_vector  = (train_vector - np.mean(train_vector)) / (10.0 * np.std(train_vector))\n","\n","    ###TEST DATA###\n","    test_label = np.array(test['y'][0])\n","    test_vector = np.array(test['x']) / 255\n","    return train_label, train_vector, test_label, test_vector\n","#**************ONE HOT*********************************************************\n","def oneHot(label, class_size):\n","    y = label.shape[0]\n","    label_matrix = np.zeros([y, class_size], dtype=int)\n","    for i in range(y):\n","        correct_class = label[i]\n","        label_matrix[i][correct_class] = 1\n","    return label_matrix\n","#**********************SIGMOID FUNCTION****************************************\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","def derivative_sigmoid(x):\n","    return sigmoid(x) * (1 - sigmoid(x))\n","# ************************ReLu FUNCTION****************************************\n","def relu(x):\n","    return np.maximum(x, 0)\n","\n","def derivative_relu(x):\n","    x[x <= 0] = 0\n","    x[x > 0] = 1\n","    return x\n","#*************************TANH FUNCTION****************************************\n","def tanh(X):\n","    return np.tanh(X)\n","    \n","def derivative_tanh(x):\n","    return 1.0 - np.tanh(x) ** 2\n","# ***************SOFTMAX FUNCTION**********************************************\n","def softmax(out_array):\n","    for i in range(out_array.shape[0]):\n","        out_array[i] = np.exp(out_array[i]) / np.sum(np.exp(out_array[i]), axis=0)\n","    return out_array\n","\n","def derivative_softmax(preds):\n","    return preds * (1 - preds)\n","#*****************LOSS FUNCTION************************************************\n","def loss(train_label, a1, size):\n","    result = -np.sum(np.multiply(train_label, np.log(a1)) + np.multiply(1 - train_label, np.log(1 - a1))) / size\n","    return result\n","# ******************************************************************************\n","# update parameters\n","def update_parameters_NN(parameters, grads, learning_rate=0.01):\n","    parameters = {\"weight1\": parameters[\"weight1\"] - learning_rate * grads[\"dweight1\"],\n","                  \"bias1\": parameters[\"bias1\"] - learning_rate * grads[\"dbias1\"],\n","                  \"weight2\": parameters[\"weight2\"] - learning_rate * grads[\"dweight2\"],\n","                  \"bias2\": parameters[\"bias2\"] - learning_rate * grads[\"dbias2\"]}\n","    return parameters\n","#################################################################################\n","# intialize parameters and layer sizes\n","def initialize_parameters_and_layer_sizes_NN(input_s, output_s, hidden_size):\n","    parameters = {\"weight1\": np.random.randn(input_s, hidden_size),\n","                  \"bias1\": 0.0,\n","                  \"weight2\": np.random.randn(hidden_size, output_s),\n","                  \"bias2\": 0.0}\n","    return parameters\n","#################################################################################\n","def forward_propagation_NN( train_vector, parameters):\n","    # this formula : oi = wijxj + bi\n","    Z1 = train_vector.dot(parameters[\"weight1\"]) + parameters[\"bias1\"]\n","    A1 = softmax(Z1/100)\n","    Z2 = A1.dot(parameters[\"weight2\"]) + parameters[\"bias2\"]\n","    A2 = softmax(Z2/100)\n","\n","    cache = {\"Z1\": Z1,#h1\n","             \"A1\": A1,#a1\n","             \"Z2\": Z2,\n","             \"A2\": A2}\n","\n","    return A2, cache\n","#################################################################################\n","# Compute cost\n","def compute_cost_NN(A2, Y, size):\n","    logprobs = np.multiply(np.log(A2),Y)\n","    cost = -np.sum(logprobs)/size\n","    return cost\n","#################################################################################\n","# Backward Propagation\n","def backward_propagation_NN(parameters, cache, X, Y,size):\n","    da2 = -((np.divide(Y,cache[\"A2\"]) - np.divide(1-Y,1-cache[\"A2\"])))/100\n","    dh2 = da2 * derivative_softmax(cache[\"A2\"])\n","    dw2 = cache[\"A1\"].T.dot(dh2)\n","    db2 = np.sum(dh2)\n","    dh1 = derivative_softmax(cache[\"Z1\"]) * db2\n","    dw1 = np.matmul(X.T, dh1)\n","    db1 = np.sum(dh1)\n","    grads = {\"dweight1\": dw1,\n","             \"dbias1\": db1,\n","             \"dweight2\": dw2,\n","             \"dbias2\": db2}\n","    return grads\n","##################################################################################\n","def accuracy(x_test,y_test,parameters):\n","    preds = softmax((parameters[\"weight1\"]) + parameters[\"bias1\"] / 100)\n","    preds2 = softmax(preds.dot(parameters[\"weight2\"]) + parameters[\"bias2\"] / 100)\n","    count = 0\n","    for i, j in zip(y_test, np.argmax(preds2, axis=1)):\n","        if i == j:\n","            count += 1\n","    print(\"result accuracy\")\n","    print(count / y_test.shape[0] * 100)\n","##################################################################################\n","def m_nn(x_train, y_train, x_test, y_test, epoch, learning_rate,batch_size,output_size, input_size , hidden_size):\n","\n","    # initialize parameters and layer sizes\n","    parameters = initialize_parameters_and_layer_sizes_NN(input_size,output_size,hidden_size)\n","    for i in range(epoch):\n","        # forward propagation\n","        size = x_train.shape[0]\n","        A2, cache = forward_propagation_NN(x_train, parameters)\n","        # compute cost\n","        compute_cost_NN(A2, y_train,size)\n","        # backward propagation\n","        grads = backward_propagation_NN(parameters,cache, x_train, y_train,size)\n","        # update parameters\n","        parameters = update_parameters_NN(parameters, grads)\n","        '''\n","        if i % 100 == 0:\n","            preds = softmax((parameters[\"weight1\"]) + parameters[\"bias1\"] / 100)\n","            preds2 = softmax(preds.dot(parameters[\"weight2\"]) + parameters[\"bias2\"] /100)\n","            count = 0\n","            for i, j in zip(y_test, np.argmax(preds2, axis=1)):\n","                if i == j:\n","                    count += 1\n","\n","            print(count / y_test.shape[0] * 100)\n","        '''\n","    accuracy(x_test, y_test,parameters)\n","\n","def main():\n","    # ********ASSIGN VALUE*******************************\n","    train_label, train_vector, test_label, test_vector = readData('train.mat', 'test.mat')\n","    epoch_size = 1000\n","    train_label = oneHot(train_label, 5)\n","    learning_rate = 0.01\n","    batch_size = 126\n","    output_size = 5\n","    input_size = 768\n","    hidden_size = 50\n","    # other typhical value 0.001, 0.01, 0.03, 0.1, 0.3, 1 etc.\n","    m_nn(train_vector, train_label, test_vector, test_label, epoch_size,learning_rate, batch_size, output_size, input_size , hidden_size)\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":0,"outputs":[]}]}